model_name: "unsloth/gpt-oss-20b"
max_seq_length: 1024
load_in_4bit: true
full_finetuning: false
dtype: null

lora:
  r: 8
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
  lora_alpha: 16
  lora_dropout: 0.0
  bias: "none"
  use_gradient_checkpointing: "unsloth"
  random_state: 3407
  use_rslora: false
  loftq_config: null

dataset:
  hub_id: "HuggingFaceH4/Multilingual-Thinking"
  split: "train"

training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  warmup_steps: 5
  max_steps: 30
  learning_rate: 0.0002
  logging_steps: 1
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "linear"
  seed: 3407
  output_dir: "outputs"
  report_to: "none"

inference:
  system_prompt: "You are a helpful assistant that can solve mathematical problems."
  user_prompt: "Solve x^5 + 3x^4 - 10 = 3."
  reasoning_effort: "medium"
  max_new_tokens: 128
  temperature: 1.0
  top_p: 1.0
